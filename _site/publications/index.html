<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Research | Jessie Finocchiaro </title> <meta name="author" content="Jessie Finocchiaro"> <meta name="description" content="Homepage for Jessie Finocchiaro "> <meta name="keywords" content="jessie finocchiaro, jessica finocchiaro, computer science"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="jfinocchiar.girhub.io/publications/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Jessie Finocchiaro </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">Home </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Research <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">People </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Running </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://photos.google.com/share/AF1QipMpu6nX-BKVFOyij2MoFXodL3qJlvxzKw2jdZKzY-udr6EfjtbCJKQZjwnu0TypuA?key=NElQc0NuZ2pUNkticUtKUWZPajZtcHAyQnEyN2xn" rel="external nofollow noopener" target="_blank">Colorado</a> <a class="dropdown-item " href="https://photos.google.com/share/AF1QipN_zfk6KqX058FocBoa8l9tpAHGxbloteZ7IFsZad2JHSu-Mc-8Vl0WSMAgOjsQLQ?key=M0NVOXROUlg0UnJFSVpZYWJjc19zaGw5bjNiaDVB" rel="external nofollow noopener" target="_blank">New England</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Research</h1> <p class="post-description"></p> </header> <article> <div class="publications"> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li><div class="row"> <div class="col-sm-2 abbr"> </div> <div id="russo2024conversations" class="col-sm-8"> <div class="title">Bridging Research and Practice Through Conversation: Reflecting on Our Experience</div> <div class="author"> Mayra Russo , Mackenzie Jorgensen , Kristen M. Scott , and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Wendy Xu, Di H. Nguyen, Jessie Finocchiaro, Matthew Olckers' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '5'); ">4 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the 2024 ACM Conference on Equity and Access to Algorithms, Mechanisms, and Optimization</em> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2409.05880" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>While some research fields have a long history of collaborating with domain experts outside academia, many quantitative researchers do not have natural avenues to meet experts in areas where the research is later deployed. We explain how conversations—interviews without a specific research objective—can bridge research and practice. Using collaborative autoethnography, we reflect on our experience of conducting conversations with practitioners from a range of different backgrounds, including refugee rights, conservation, addiction counseling, and municipal data science. Despite these varied backgrounds, common lessons emerged, including the importance of valuing the knowledge of experts, recognizing that academic research and practice have differing objectives and timelines, understanding the limits of quantification, and avoiding data extractivism. We consider the impact of these conversations on our work, the potential roles we can serve as researchers, and the challenges we anticipate as we move forward in these collaborations.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">russo2024conversations</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Bridging Research and Practice Through Conversation: Reflecting on Our Experience}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Russo, Mayra and Jorgensen, Mackenzie and Scott, Kristen M. and Xu, Wendy and Nguyen, Di H. and Finocchiaro, Jessie and Olckers, Matthew}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 2024 ACM Conference on Equity and Access to Algorithms, Mechanisms, and Optimization}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div></li> <li><div class="row"> <div class="col-sm-2 abbr"> </div> <div id="finocchiaro2024embeddingJMLR" class="col-sm-8"> <div class="title">An Embedding Framework for the Design and Analysis of Consistent Polyhedral Surrogates</div> <div class="author"> <em>Jessie Finocchiaro</em>, Rafael M Frongillo , and Bo Waggoner </div> <div class="periodical"> <em>Journal of Machine Learning Research</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2206.14707" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.youtube.com/watch?v=IhGuusxuLLE" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>We formalize and study the natural approach of designing convex surrogate loss functions via embeddings, for problems such as classification, ranking, or structured prediction. In this approach, one embeds each of the finitely many predictions (e.g. rankings) as a point in Rd, assigns the original loss values to these points, and "convexifies" the loss in some way to obtain a surrogate. We establish a strong connection between this approach and polyhedral (piecewise-linear convex) surrogate losses: every discrete loss is embedded by some polyhedral loss, and every polyhedral loss embeds some discrete loss. Moreover, an embedding gives rise to a consistent link function as well as linear surrogate regret bounds. Our results are constructive, as we illustrate with several examples. In particular, our framework gives succinct proofs of consistency or inconsistency for various polyhedral surrogates in the literature, and for inconsistent surrogates, it further reveals the discrete losses for which these surrogates are consistent. We go on to show additional structure of embeddings, such as the equivalence of embedding and matching Bayes risks, and the equivalence of various notions of non-redudancy. Using these results, we establish that indirect elicitation, a necessary condition for consistency, is also sufficient when working with polyhedral surrogates.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">finocchiaro2024embeddingJMLR</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{An Embedding Framework for the Design and Analysis of Consistent Polyhedral Surrogates}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Finocchiaro, Jessie and Frongillo, Rafael M and Waggoner, Bo}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Journal of Machine Learning Research}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <div class="abstract hidden"> <div style="text-align: center;"> <figure> <iframe src="https://www.youtube.com/watch?v=IhGuusxuLLE" class="img-fluid rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="auto" height="auto"></iframe> </figure> </div> </div> </div> </div></li> <li><div class="row"> <div class="col-sm-2 abbr"> </div> <div id="finocchiaro2024fairelic" class="col-sm-8"> <div class="title">Using Property Elicitation to Understand the Impacts of Fairness Constraints</div> <div class="author"> <em>Jessie Finocchiaro</em> </div> <div class="periodical"> <em>In ACM Conference on Fairness, Accountability, and Transparency (FAccT)</em> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2309.11343" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/GIW_EC.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="abstract hidden"> <p>Predictive algorithms are often trained by optimizing some loss function, to which regularization functions are added to impose a penalty for violating constraints. As expected, the addition of such regularization functions can change the minimizer of the objective. It is not well-understood which regularizers change the minimizer of the loss, and, when the minimizer does change, how it changes. We use property elicitation to take first steps towards understanding the joint relationship between the loss and regularization functions and the optimal decision for a given problem instance. In particular, we give a necessary and sufficient condition on loss and regularizer pairs for when a property changes with the addition of the regularizer, and examine some regularizers satisfying this condition standard in the fair machine learning literature. We empirically demonstrate how algorithmic decision-making changes as a function of both data distribution changes and hardness of the constraints.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">finocchiaro2024fairelic</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Using Property Elicitation to Understand the Impacts of Fairness Constraints}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Finocchiaro, Jessie}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ACM Conference on Fairness, Accountability, and Transparency (FAccT)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div></li> <li><div class="row"> <div class="col-sm-2 abbr"> </div> <div id="nueve2024tradeoffs" class="col-sm-8"> <div class="title">Trading off Consistency and Dimensionality of Convex Surrogates for the Mode</div> <div class="author"> Enrique Nueve , Dhamma Kimpara , Bo Waggoner , and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Jessie Finocchiaro' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '5'); ">1 more author</span> </div> <div class="periodical"> <em>In Neural Information Processing Systems</em> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2402.10818" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>In multiclass classification over n outcomes, the outcomes must be embedded into the reals with dimension at least n−1 in order to design a consistent surrogate loss that leads to the "correct" classification, regardless of the data distribution. For large n, such as in information retrieval and structured prediction tasks, optimizing a surrogate in n−1 dimensions is often intractable. We investigate ways to trade off surrogate loss dimension, the number of problem instances, and restricting the region of consistency in the simplex for multiclass classification. Following past work, we examine an intuitive embedding procedure that maps outcomes into the vertices of convex polytopes in a low-dimensional surrogate space. We show that full-dimensional subsets of the simplex exist around each point mass distribution for which consistency holds, but also, with less than n−1 dimensions, there exist distributions for which a phenomenon called hallucination occurs, which is when the optimal report under the surrogate loss is an outcome with zero probability. Looking towards application, we derive a result to check if consistency holds under a given polytope embedding and low-noise assumption, providing insight into when to use a particular embedding. We provide examples of embedding n=2d outcomes into the d-dimensional unit cube and n=d! outcomes into the d-dimensional permutahedron under low-noise assumptions. Finally, we demonstrate that with multiple problem instances, we can learn the mode with n2 dimensions over the whole simplex.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">nueve2024tradeoffs</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Trading off Consistency and Dimensionality of Convex Surrogates for the Mode}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Nueve, Enrique and Kimpara, Dhamma and Waggoner, Bo and Finocchiaro, Jessie}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Neural Information Processing Systems}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div></li> <li><div class="row"> <div class="col-sm-2 abbr"> </div> <div id="shirali2024participatory" class="col-sm-8"> <div class="title">Participatory Objective Design via Preference Elicitation</div> <div class="author"> Ali Shirali , <em>Jessie Finocchiaro</em>, and Rediet Abebe </div> <div class="periodical"> <em>In ACM Conference on Fairness, Accountability, and Transparency (FAccT)</em> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>In standard resource allocation problems, the designer sets the objective—such as utilitarian social welfare—that captures a societal goal and solves for the optimal allocation subject to fairness and item availability constraints. The participants, on the other hand, specify their preferences for the items being allocated, e.g., through stating how they rank the items or expressing their cardinal utility for each item. The objective function, which guides the overall allocation, is therefore determined by the designer in a top-down manner, whereas participants can only express their preferences for the items. This standard preference elicitation stage limits participants’ ability to express preferences for the overall allocation, such as the level of inequality, and influence the overall objective function. In this work, we examine whether it is possible to use this bottom-up preference elicitation stage to enable participants to express not only their preferences for individual items but also their preferences for the overall allocation, thereby indirectly influencing the objective function. We examine this question using a well-studied resource allocation problem where m divisible items must be allocated to n agents, who express their cardinal utilities over the items. The designer aims to optimize for the sum of the agents’ utilities for the items they receive. In particular, this utilitarian objective is agnostic to the overall inequality level. We consider a setting where the agents’ true utility is a function not only of their preferences for the items, but also the overall level of inequality. We model this using a popular social preference model from behavioral economics by Fehr and Schmidt, where agents can express levels of inequality aversion. We conduct a theoretical examination of this problem and show that there can be large gains in social welfare if the designer uses this richer inequality-aware preference model, instead of the standard inequality-agnostic preference model. Further, if we take the standard inequality-agnostic welfare as the benchmark, we show that the relative loss of welfare can be tightly bounded–shown to be independent of the number of agents and linear in the level of inequality aversion. With further assumptions on the preferences, we provide strictly tighter, distribution-free, and parametric bounds on the loss of welfare. We also discuss the worst-case drop in inequality-agnostic utility an agent might incur as a consequence of a designer allocating items using the inequality-averse preferences. We conclude with a discussion on possible designs to elicit the preferences of strategic agents over the goods and fairness. Taken together, our results argue for potentially large gains that can be obtained from using the richer social preference model and demonstrate the relatively minor losses from using the standard model, highlighting a promising avenue for using preference elicitation to empower participants to influence the overall objective function.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">shirali2024participatory</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Participatory Objective Design via Preference Elicitation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Shirali, Ali and Finocchiaro, Jessie and Abebe, Rediet}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ACM Conference on Fairness, Accountability, and Transparency (FAccT)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://dl.acm.org/doi/10.1145/3630106.3658994}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div></li> <li><div class="row"> <div class="col-sm-2 abbr"> </div> <div id="chen204robustness" class="col-sm-8"> <div class="title">Robustness of voting mechanisms to external information in expectation</div> <div class="author"> Yiling Chen , and <em>Jessie Finocchiaro</em> </div> <div class="periodical"> <em></em> 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2404.07818" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Analyses of voting algorithms often overlook informational externalities shaping individual votes. For example, pre-polling information often skews voters towards candidates who may not be their top choice, but who they believe would be a worthwhile recipient of their vote. In this work, we aim to understand the role of external information in voting outcomes. We study this by analyzing (1) the probability that voting outcomes align with external information, and (2) the effect of external information on the total utility across voters, or social welfare. In practice, voting mechanisms elicit coarse information about voter utilities, such as ordinal preferences, which initially prevents us from directly analyzing the effect of informational externalities with standard voting mechanisms. To overcome this, we present an intermediary mechanism for learning how preferences change with external information which does not require eliciting full cardinal preferences. With this tool in hand, we find that voting mechanisms are generally more likely to select the alternative most favored by the external information, and when external information reflects the population’s true preferences, social welfare increases in expectation.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">chen204robustness</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Robustness of voting mechanisms to external information in expectation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chen, Yiling and Finocchiaro, Jessie}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div></li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li><div class="row"> <div class="col-sm-2 abbr"> </div> <div id="schoeffer2023online" class="col-sm-8"> <div class="title">Online platforms and the fair exposure problem under homophily</div> <div class="author"> Jakob Schoeffer , Alexander Ritchie , Keziah Naggita , and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Faidra Monachou, Jessie Finocchiaro, Marc Juarez' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '5'); ">3 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the AAAI Conference on Artificial Intelligence</em> , 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2202.09727" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/INFORMS-fair-exposure.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="abstract hidden"> <p>In the wake of increasing political extremism, online platforms have been criticized for contributing to polarization. One line of criticism has focused on echo chambers and the recommended content served to users by these platforms. In this work, we introduce the fair exposure problem: given limited intervention power of the platform, the goal is to enforce balance in the spread of content (e.g., news articles) among two groups of users through constraints similar to those imposed by the Fairness Doctrine in the United States in the past. Groups are characterized by different affiliations (e.g., political views) and have different preferences for content. We develop a stylized framework that models intra- and intergroup content propagation under homophily, and we formulate the platform’s decision as an optimization problem that aims at maximizing user engagement, potentially under fairness constraints. Our main notion of fairness requires that each group see a mixture of their preferred and non-preferred content, encouraging information diversity. Promoting such information diversity is often viewed as desirable and a potential means for breaking out of harmful echo chambers. We study the solutions to both the fairness-agnostic and fairness-aware problems. We prove that a fairness-agnostic approach inevitably leads to group-homogeneous targeting by the platform. This is only partially mitigated by imposing fairness constraints: we show that there exist optimal fairness-aware solutions which target one group with different types of content and the other group with only one type that is not necessarily the group’s most preferred. Finally, using simulations with real-world data, we study the system dynamics and quantify the price of fairness.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">schoeffer2023online</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Online platforms and the fair exposure problem under homophily}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Schoeffer, Jakob and Ritchie, Alexander and Naggita, Keziah and Monachou, Faidra and Finocchiaro, Jessie and Juarez, Marc}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the AAAI Conference on Artificial Intelligence}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{37}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{10}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{11899--11908}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div></li> <li><div class="row"> <div class="col-sm-2 abbr"> </div> <div id="johnsonyu2023characterizing" class="col-sm-8"> <div class="title">Characterizing and Improving the Robustness of Predict-Then-Optimize Frameworks</div> <div class="author"> Sonja Johnson-Yu , <em>Jessie Finocchiaro</em>, Kai Wang , and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Yevgeniy Vorobeychik, Arunesh Sinha, Aparna Taneja, Milind Tambe' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '5'); ">4 more authors</span> </div> <div class="periodical"> <em>In International Conference on Decision and Game Theory for Security</em> , 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/PTO-robustness.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Optimization tasks situated in incomplete information settings are often preceded by a prediction problem to estimate the missing information; past work shows the traditional predict-then-optimize (PTO) framework can be improved by training a predictive model with respect to the optimization task through a PTO paradigm called decision-focused learning. Little is known, however, about the performance of traditional PTO and decision-focused learning when exposed to adversarial label drift. We provide modifications of traditional PTO and decision-focused learning that attempt to improve robustness by anticipating label drift. When the predictive model is perfectly expressive, we cast these learning problems as Stackelberg games. With these games, we provide a necessary condition for when anticipating label drift can improve the performance of a PTO algorithm: if performance can be improved, then the downstream optimization objective must be asymmetric. We then bound the loss of decision quality in the presence of adversarial label drift to show there may exist a strict gap between the performance of the two algorithms. We verify our theoretical findings empirically in two asymmetric and two symmetric settings. These experimental results demonstrate that robustified decision-focused learning is generally more robust to adversarial label drift than both robust and traditional PTO.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">johnsonyu2023characterizing</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Characterizing and Improving the Robustness of Predict-Then-Optimize Frameworks}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Johnson-Yu, Sonja and Finocchiaro, Jessie and Wang, Kai and Vorobeychik, Yevgeniy and Sinha, Arunesh and Taneja, Aparna and Tambe, Milind}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Decision and Game Theory for Security}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{133--152}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{Springer}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div></li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li><div class="row"> <div class="col-sm-2 abbr"> </div> <div id="finocchiaro2022topk" class="col-sm-8"> <div class="title">Consistent polyhedral surrogates for top-k classification and variants</div> <div class="author"> <em>Jessie Finocchiaro</em>, Rafael Frongillo , Emma Goodwill , and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Anish Thilagar' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '5'); ">1 more author</span> </div> <div class="periodical"> <em>In International Conference on Machine Learning</em> , 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2207.08873" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Top-k classification is a generalization of multiclass classification used widely in information retrieval, image classification, and other extreme classification settings. Several hinge-like (piecewise-linear) surrogates have been proposed for the problem, yet all are either non-convex or inconsistent. For the proposed hinge-like surrogates that are convex (i.e., polyhedral), we apply the recent embedding framework of Finocchiaro et al. (2019; 2022) to determine the prediction problem for which the surrogate is consistent. These problems can all be interpreted as variants of top-k classification, which may be better aligned with some applications. We leverage this analysis to derive constraints on the conditional label distributions under which these proposed surrogates become consistent for top-k. It has been further suggested that every convex hinge-like surrogate must be inconsistent for top-k. Yet, we use the same embedding framework to give the first consistent polyhedral surrogate for this problem.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">finocchiaro2022topk</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Consistent polyhedral surrogates for top-k classification and variants}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Finocchiaro, Jessie and Frongillo, Rafael and Goodwill, Emma and Thilagar, Anish}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Machine Learning}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{21329--21359}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{PMLR}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div></li> <li><div class="row"> <div class="col-sm-2 abbr"> </div> <div id="finocchiaro2022lovasz" class="col-sm-8"> <div class="title">The structured abstain problem and the lovász hinge</div> <div class="author"> <em>Jessie Finocchiaro</em>, Rafael M. Frongillo , and Enrique Nueve </div> <div class="periodical"> <em>In Conference on Learning Theory</em> , 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2203.08645" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>The Lovász hinge is a convex surrogate recently proposed for structured binary classification, in which k binary predictions are made simultaneously and the error is judged by a submodular set function. Despite its wide usage in image segmentation and related problems, its consistency has remained open. We resolve this open question, showing that the Lovász hinge is inconsistent for its desired target unless the set function is modular. Leveraging a recent embedding framework, we instead derive the target loss for which the Lovász hinge is consistent. This target, which we call the structured abstain problem, allows one to abstain on any subset of the k predictions. We derive two link functions, each of which are consistent for all submodular set functions simultaneously.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">finocchiaro2022lovasz</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{The structured abstain problem and the lov{\'a}sz hinge}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Finocchiaro, Jessie and Frongillo, Rafael M. and Nueve, Enrique}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Learning Theory}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3718--3740}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{PMLR}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div></li> <li><div class="row"> <div class="col-sm-2 abbr"> </div> <div id="finocchiaro2022designing" class="col-sm-8"> <div class="title">Designing Consistent and Convex Surrogates for General Prediction Tasks</div> <div class="author"> <em>Jessie Finocchiaro</em> </div> <div class="periodical"> <em>University of Colorado at Boulder</em> , 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/thesis-finocchiaro.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Supervised machine learning algorithms are often predicated on the minimization of loss functions which measure error of a given prediction against a ground truth label. The choice of loss function to minimize corresponds to a summary statistic of the underlying data distribution that is learned in this process. Historically, loss function design has often been ad-hoc, and often results in losses that are not actually statistically consistent with respect to the target prediction task. This work focuses on the design of losses that are simultaneously convex, consistent with respect to a target prediction task, and efficient in the dimension of the prediction space. We provide frameworks to construct such losses in both discrete prediction and continuous estimation settings, as well as tools to lower bound the prediction dimension for certain classes of consistent convex losses. We apply our results throughout to understand prediction tasks such as high-confidence classification, top-k prediction, variance estimation, conditional value at risk, and ratios of expectations.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@phdthesis</span><span class="p">{</span><span class="nl">finocchiaro2022designing</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Designing Consistent and Convex Surrogates for General Prediction Tasks}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Finocchiaro, Jessie}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">school</span> <span class="p">=</span> <span class="s">{University of Colorado at Boulder}</span><span class="p">,</span>
  <span class="na">selected_publication</span> <span class="p">=</span> <span class="s">{True}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div></li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li><div class="row"> <div class="col-sm-2 abbr"> </div> <div id="finocchiaro2021bridging" class="col-sm-8"> <div class="title">Bridging machine learning and mechanism design towards algorithmic fairness</div> <div class="author"> <em>Jessie Finocchiaro</em>, Roland Maio , Faidra Monachou , and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Gourab K Patro, Manish Raghavan, Ana-Andreea Stoica, Stratis Tsirtsis' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '5'); ">4 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency</em> , 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2010.05434" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.youtube.com/watch?v=BDgyLPVHB4A&amp;list=PLI0o-KVQWwQ_PushsqsnMwkkhbWXhVnD-&amp;index=12" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>Decision-making systems increasingly orchestrate our world: how to intervene on the algorithmic components to build fair and equitable systems is therefore a question of utmost importance; one that is substantially complicated by the context-dependent nature of fairness and discrimination. Modern decision-making systems that involve allocating resources or information to people (e.g., school choice, advertising) incorporate machine-learned predictions in their pipelines, raising concerns about potential strategic behavior or constrained allocation, concerns usually tackled in the context of mechanism design. Although both machine learning and mechanism design have developed frameworks for addressing issues of fairness and equity, in some complex decision-making systems, neither framework is individually sufficient. In this paper, we develop the position that building fair decision-making systems requires overcoming these limitations which, we argue, are inherent to each field. Our ultimate objective is to build an encompassing framework that cohesively bridges the individual frameworks of mechanism design and machine learning. We begin to lay the ground work towards this goal by comparing the perspective each discipline takes on fair decision-making, teasing out the lessons each field has taught and can teach the other, and highlighting application domains that require a strong collaboration between these disciplines.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">finocchiaro2021bridging</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Bridging machine learning and mechanism design towards algorithmic fairness}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Finocchiaro, Jessie and Maio, Roland and Monachou, Faidra and Patro, Gourab K and Raghavan, Manish and Stoica, Ana-Andreea and Tsirtsis, Stratis}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{489--503}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <div class="abstract hidden"> <div style="text-align: center;"> <figure> <iframe src="https://www.youtube.com/watch?v=BDgyLPVHB4A&amp;list=PLI0o-KVQWwQ_PushsqsnMwkkhbWXhVnD-&amp;index=12" class="img-fluid rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="auto" height="auto"></iframe> </figure> </div> </div> </div> </div></li> <li><div class="row"> <div class="col-sm-2 abbr"> </div> <div id="finocchiaro2021unifying" class="col-sm-8"> <div class="title">Unifying lower bounds on prediction dimension of convex surrogates</div> <div class="author"> <em>Jessie Finocchiaro</em>, Rafael M. Frongillo , and Bo Waggoner </div> <div class="periodical"> <em>Advances in Neural Information Processing Systems</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2102.08218" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/cvx-flats-neurips21-poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="abstract hidden"> <p>Given a prediction task, understanding when one can and cannot design a consistent convex surrogate loss, particularly a low-dimensional one, is an important and active area of machine learning research. The prediction task may be given as a target loss, as in classification and structured prediction, or simply as a (conditional) statistic of the data, as in risk measure estimation. These two scenarios typically involve different techniques for designing and analyzing surrogate losses. We unify these settings using tools from property elicitation, and give a general lower bound on prediction dimension. Our lower bound tightens existing results in the case of discrete predictions, showing that previous calibration-based bounds can largely be recovered via property elicitation. For continuous estimation, our lower bound resolves on open problem on estimating measures of risk and uncertainty.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">finocchiaro2021unifying</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Unifying lower bounds on prediction dimension of convex surrogates}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Finocchiaro, Jessie and Frongillo, Rafael M. and Waggoner, Bo}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Advances in Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{34}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{22046--22057}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div></li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"> <li><div class="row"> <div class="col-sm-2 abbr"> </div> <div id="finocchiaro2020embedding" class="col-sm-8"> <div class="title">Embedding dimension of polyhedral losses</div> <div class="author"> <em>Jessie Finocchiaro</em>, Rafael M. Frongillo , and Bo Waggoner </div> <div class="periodical"> <em>In Conference on Learning Theory</em> , 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/embed-dim.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://www.learningtheory.org/colt2020/virtual/papers/paper_369.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>A common technique in supervised learning with discrete losses, such as 0-1 loss, is to optimize a convex surrogate loss over $\mathbbR^d, calibrated with respect to the original loss. In particular, recent work has investigated embedding the original predictions (e.g. labels) as points in \mathbbR^d, showing an equivalence to using polyhedral surrogates. In this work, we study the notion of the embedding dimension of a given discrete loss: the minimum dimension d such that an embedding exists. We characterize d-embeddability for all d, with a particularly tight characterization for d=1$ (embedding into the real line), and useful necessary conditions for d&gt;1 in the form of a quadratic feasibility program. We illustrate our results with novel lower bounds for abstain loss.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">finocchiaro2020embedding</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Embedding dimension of polyhedral losses}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Finocchiaro, Jessie and Frongillo, Rafael M. and Waggoner, Bo}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Learning Theory}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1558--1585}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{PMLR}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <div class="abstract hidden"> <div style="text-align: center;"> <figure> <iframe src="https://www.learningtheory.org/colt2020/virtual/papers/paper_369.html" class="img-fluid rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="auto" height="auto"></iframe> </figure> </div> </div> </div> </div></li> <li><div class="row"> <div class="col-sm-2 abbr"> </div> <div id="back2020decomposition" class="col-sm-8"> <div class="title">A Decomposition of a Complete Graph with a Hole</div> <div class="author"> Roxanne Back , Alejandra Brewer Castano , Rachel Galindo , and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Jessie Finocchiaro' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '5'); ">1 more author</span> </div> <div class="periodical"> <em>Open Journal of Discrete Mathematics</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/decompose-graph-holes.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>In the field of design theory, the most well-known design is a Steiner Triple System. In general, a G-design on H is an edge-disjoint decomposition of H into isomorphic copies of G. In a Steiner Triple system, a complete graph is decomposed into triangles. In this paper we let H be a complete graph with a hole and G be a complete graph on four vertices minus one edge, also referred to as a K4 −e. A complete graph with a hole, Kd +v, consists of a complete graph on d vertices, Kd , and a set of independent vertices of size v, V, where each vertex in V is adjacent to each vertex in Kd . When d is even, we give two constructions for the decomposition of a complete graph with a hole into copies of K4 − e : the Alpha-Delta Construction, and the Alpha- Beta-Delta Construction. By restricting d and v so that v = 2(d 1−) 5−a , we are able to resolve both of these cases for a subset of Kd + v using difference methods and 1-factors.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">back2020decomposition</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Decomposition of a Complete Graph with a Hole}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Back, Roxanne and Castano, Alejandra Brewer and Galindo, Rachel and Finocchiaro, Jessie}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Open Journal of Discrete Mathematics}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{11}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--12}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Scientific Research Publishing}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div></li> <li><div class="row"> <div class="col-sm-2 abbr"> </div> <div id="finocchiaro2020evolutionary" class="col-sm-8"> <div class="title">Evolutionary Optimization of Cooperative Strategies for the Iterated Prisoner’s Dilemma</div> <div class="author"> <em>Jessie Finocchiaro</em>, and H David Mathias </div> <div class="periodical"> <em>IEEE Transactions on Games</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/tog-coop-ipd.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>The Iterated Prisoner’s Dilemma (IPD) has been studied in fields as diverse as economics, computer science, psychology, politics, and environmental studies. This is due, in part, to the intriguing property that its Nash Equilibrium is not globally optimal. Typically treated as a single-objective problem, a player’s goal is to maximize their own score. In some work, minimizing the opponent’s score is an additional objective. Here, we explore the role of explicitly optimizing for mutual cooperation in IPD player performance. We implement a genetic algorithm in which each member of the population evolves using one of four multi-objective fitness functions: selfish, communal, cooperative, and selfless, the last three of which use a cooperative metric as an objective. As a control, we also consider two single- objective fitness functions. We explore the role of representation in evolving cooperation by implementing four representations for evolving players. Finally, we evaluate the effect of noise on the evolution of cooperative behaviors. Testing our evolved players in tournaments in which a player’s own score is the sole metic, we find that players evolved with mutual cooperation as an objective are very competitive. Thus, learning to play nicely with others is a successful strategy for maximizing personal reward.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">finocchiaro2020evolutionary</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Evolutionary Optimization of Cooperative Strategies for the Iterated Prisoner's Dilemma}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Finocchiaro, Jessie and Mathias, H David}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Games}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{13}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{2}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{170--179}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div></li> </ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"> <li><div class="row"> <div class="col-sm-2 abbr"> </div> <div id="finocchiaro2019embedding" class="col-sm-8"> <div class="title">An embedding framework for consistent polyhedral surrogates</div> <div class="author"> <em>Jessie Finocchiaro</em>, Rafael Frongillo , and Bo Waggoner </div> <div class="periodical"> <em>Advances in neural information processing systems</em>, 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1907.07330" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.youtube.com/watch?v=IhGuusxuLLE" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>We formalize and study the natural approach of designing convex surrogate loss functions via embeddings, for problems such as classification, ranking, or structured prediction. In this approach, one embeds each of the finitely many predictions (e.g. rankings) as a point in ℝd, assigns the original loss values to these points, and "convexifies" the loss in some way to obtain a surrogate. We establish a strong connection between this approach and polyhedral (piecewise-linear convex) surrogate losses. Given any polyhedral loss L, we give a construction of a link function through which L is a consistent surrogate for the loss it embeds. Conversely, we show how to construct a consistent polyhedral surrogate for any given discrete loss. Our framework yields succinct proofs of consistency or inconsistency of various polyhedral surrogates in the literature, and for inconsistent surrogates, it further reveals the discrete losses for which these surrogates are consistent. We show some additional structure of embeddings, such as the equivalence of embedding and matching Bayes risks, and the equivalence of various notions of non-redudancy. Using these results, we establish that indirect elicitation, a necessary condition for consistency, is also sufficient when working with polyhedral surrogates.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">finocchiaro2019embedding</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{An embedding framework for consistent polyhedral surrogates}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Finocchiaro, Jessie and Frongillo, Rafael and Waggoner, Bo}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Advances in neural information processing systems}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{32}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <div class="abstract hidden"> <div style="text-align: center;"> <figure> <iframe src="https://www.youtube.com/watch?v=IhGuusxuLLE" class="img-fluid rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="auto" height="auto"></iframe> </figure> </div> </div> </div> </div></li> <li><div class="row"> <div class="col-sm-2 abbr"> </div> <div id="finocchiaro2019IPD" class="col-sm-8"> <div class="title">Evolving cooperation for the iterated prisoner’s dilemma</div> <div class="author"> <em>Jessie Finocchiaro</em>, and H. David Mathias </div> <div class="periodical"> <em>In Proceedings of the Genetic and Evolutionary Computation Conference Companion</em> , 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/ipd-gecco19.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>The Iterated Prisoner’s Dilemma (IPD) is an intriguing problem for which the Nash Equilibrium is not globally optimal. Typically treated as a single-objective problem, a player’s goal is to maximize their own score. In some work, minimizing the opponent’s score has been added as an additional objective. We explore the role of mutual cooperation in IPD player performance. We implement a genetic algorithm in which the population is divided into four multi-objective sub-populations: selfish, communal, cooperative, and selfless, the last three of which use a measure of mutual cooperation as an objective. Game play occurs among all members, without regard to sub-population, while crossover and selection occur only within a sub-population. Testing is against a population of well-known strategies and is single objective, using only self score. We find that players evolved to cooperate perform very well, in some cases dominating the competition. Thus, learning to play nicely with others is a successful strategy for maximizing personal reward.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">finocchiaro2019IPD</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Finocchiaro, Jessie and Mathias, H. David}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Evolving cooperation for the iterated prisoner's dilemma}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450367486}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3319619.3322021}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3319619.3322021}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Genetic and Evolutionary Computation Conference Companion}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{199–200}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{2}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{iterated prisoner's dilemma nash equilibrium genetic algorithms multi-objective optimization}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Prague, Czech Republic}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{GECCO '19}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div></li> </ol> <h2 class="bibliography">2018</h2> <ol class="bibliography"><li><div class="row"> <div class="col-sm-2 abbr"> </div> <div id="finocchiaro2018convex" class="col-sm-8"> <div class="title">Convex elicitation of continuous properties</div> <div class="author"> <em>Jessie Finocchiaro</em>, and Rafael M. Frongillo </div> <div class="periodical"> <em>Advances in Neural Information Processing Systems</em>, 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/cvx-elic-cts-paper.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/pdf/neurips18-poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="/assets/pdf/neurips18-slides.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="abstract hidden"> <p>A property or statistic of a distribution is said to be elicitable if it can be expressed as the minimizer of some loss function in expectation. Recent work shows that continuous real-valued properties are elicitable if and only if they are identifiable, meaning the set of distributions with the same property value can be described by linear constraints. From a practical standpoint, one may ask for which such properties do there exist convex loss functions. In this paper, in a finite-outcome setting, we show that in fact essentially every elicitable real-valued property can be elicited by a convex loss function. Our proof is constructive, and leads to convex loss functions for new properties.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">finocchiaro2018convex</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Convex elicitation of continuous properties}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Finocchiaro, Jessie and Frongillo, Rafael M.}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Advances in Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{31}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div></li></ol> <h2 class="bibliography">2017</h2> <ol class="bibliography"><li><div class="row"> <div class="col-sm-2 abbr"> </div> <div id="finocchiaro2017egocentric" class="col-sm-8"> <div class="title">Egocentric height estimation</div> <div class="author"> <em>Jessie Finocchiaro</em>, Aisha Urooj Khan , and Ali Borji </div> <div class="periodical"> <em>In 2017 IEEE Winter Conference on Applications of Computer Vision (WACV)</em> , 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1610.02714" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Egocentric, or first-person vision which became popular in recent years with an emerge in wearable technology, is different than exocentric (third-person) vision in some distinguishable ways, one of which being that the camera wearer is generally not visible in the video frames. Recent work has been done on action and object recognition in egocentric videos, as well as work on biometric extraction from first-person videos. Height estimation can be a useful feature for both soft-biometrics and object tracking. Here, we propose a method of estimating the height of an egocentric camera without any calibration or reference points. We used both traditional computer vision approaches and deep learning in order to determine the visual cues that results in best height estimation. Here, we introduce a framework inspired by two stream networks comprising of two Convolutional Neural Networks, one based on spatial information, and one based on information given by optical flow in a frame. Given an egocentric video as an input to the framework, our model yields a height estimate as an output. We also incorporate late fusion to learn a combination of temporal and spatial cues. Comparing our model with other methods we used as baselines, we achieve height estimates for videos with a Mean Average Error of 14.04 cm over a range of 103 cm of data, and classification accuracy for relative height (tall, medium or short) up to 93.75% where chance level is 33%.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">finocchiaro2017egocentric</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Egocentric height estimation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Finocchiaro, Jessie and Khan, Aisha Urooj and Borji, Ali}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2017 IEEE Winter Conference on Applications of Computer Vision (WACV)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1142--1150}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div></li></ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Jessie Finocchiaro. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: November 27, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?da39b660470d1ba6e6b8bf5f37070b6e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>